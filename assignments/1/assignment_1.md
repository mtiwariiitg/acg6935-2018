# Assignment 1

The purpose of the first assignment is to use Python for 'general purpose' scripting:

- scanning directory structure/reading files from disk
- organizing data, and using (complex) data structures

## Task

The task is to read SAS logistic regression output files (each regression has 7 files), and organize these into tables (where each table can have multiple regressions). The tables are written as .csv (for further processing in Excel), and in LaTeX format (.tex).

The work is divided into three parts (one for each group).

## Group 1

Group 1 will write a script that takes a single argument (folder), and will scan that folder for all .csv file names that start with `logistic`.

You may assume that the following code is used for logistic regressions:


```SAS
%macro doLogistic(dsin=, dep=, vars=);
	proc logistic data=&dsin descending  ;
	  model &dep = &vars  d2004-d2013 ff12_1-ff12_12 / RSQUARE SCALE=none ;
	  /* not needed here, but out= captures fitted, errors, etc */
	  output out = logistic_predicted  PREDICTED=predicted ;

	  	ods output	ParameterEstimates  = _outp1
					OddsRatios 			= _outp2
					Association 		= _outp3
					RSquare  			= _outp4
					ResponseProfile 	= _outp5
					GlobalTests   		= _outp6			
					NObs 				= _outp7 ;
	%runquit;
%mend;

/* helper macro to export the 7 tables for each logistic regression */
%macro exportLogit(j, k);
	%myExport(dset=_outp1, file=&exportDir\logistic_&j._&k._coef.csv);
	%myExport(dset=_outp2, file=&exportDir\logistic_&j._&k._odds.csv);
	%myExport(dset=_outp3, file=&exportDir\logistic_&j._&k._assoc.csv);
	%myExport(dset=_outp4, file=&exportDir\logistic_&j._&k._rsqr.csv);
	%myExport(dset=_outp5, file=&exportDir\logistic_&j._&k._response.csv);
	%myExport(dset=_outp6, file=&exportDir\logistic_&j._&k._globaltest.csv);
	%myExport(dset=_outp7, file=&exportDir\logistic_&j._&k._numobs.csv);
%mend;

/*	Do logistic regression */

/* model 1 */
%doLogistic(dsin=h_large (where=(hasanymiss eq 0)), dep=loss, vars=ret beta size btm );
%exportLogit(t1,col1);

/* model 2 */
%doLogistic(dsin=h_large (where=(hasanymiss eq 0)), dep=loss, vars=ret beta_lag size_lag btm_lag );
%exportLogit(t1,col2);
```

`logistic_tX_colY_type.csv`, where `X` is a table number (1, 2, ..) and `Y` is the column number (1, 2, ..), and `type` indicates the type of output file (coef, odds, assoc, rsqr, response, globaltest, numobs). 

Example output is included in `sas_exports`.

The deliverable of group one is a list of tables, where each table is a list of columns. A column is a list of dictionaries, where each dictionary has the following properties:

- filename: string with file name
- type: string, holding 'coef', 'odds', 'assoc', 'rsqr', 'response', 'globaltest', or 'numobs'
- content: list of dictionaries (each line of each file stored in a dictionary)

## Group 2

The second group will create script that takes a table structure (generated by group 1) as the input, and which outputs a datastructure that holds a list of column datastructures, where each column datastructure holds:

- counter: a counter (the model number 1, 2, ..)
- estimates: a list of coefficients, stored as dictionaries (with keys: name, coefficient, z-value, p-value )
- numObs: integer with the number of observations 
- RSquared: float with the pseudo R-squared

This group does not have to wait until group 1 is finished; they can define a temporary datastructure and use that to write/test their code.

The temporary datastructure of group 1 output to use (a single table structure):

```python

coeff1 = {
	'filename' : 'logistic_t1_col1_coeff.csv',
	'type' : 'coeff',
	'content' : [ 
		{
			'Variable' : 'Intercept',
			'DF': 1,
			'Estimate': 1.7645,
			'StdErr': 0.1447 
			# etc
		}, 
		{
			'Variable' : 'ret',
			'DF': 1,
			'Estimate': -0.5986,
			'StdErr': 0.0587 
			# etc
		}
		# etc
		]
}

odds1 = {
	'filename' : 'logistic_t1_col1_odds.csv',
	'type' : 'coeff',
	'content' : [ 
		{
			'Effect' : 'ret',
			'OddsRatioEst': 0.320,
			'LowerCL': 0.303,
			'UpperCL': 0.337 
		}, 
		{
			'Effect' : 'beta_lag',
			'OddsRatioEst': 1.529,
			'LowerCL': 1.488,
			'UpperCL': 1.570
		}
		# etc
		]	
}
column1 = [ coeff1, odds1, assoc1, rsqr1, response1, globaltest1, numobs1 ]
column2 = [ coeff2, odds2, assoc2, rsqr2, response2, globaltest2, numobs2 ]
table = [ column1, column2 ]
```

Note: the output of group 1 is a list of tables, whereas the input for group 2 is one table.

## Group 3

Group 3 will create a script that takes  the output of group 2 and generates a csv file with a table (with each regression in one column).

This group does not have to wait until group 2 is finished; they can define a temporary datastructure and use that to write/test their code.

The temporary datastructure would look like this:

```python

column1 = {
	'counter' : 1,	
	'estimates' : [
		{ 'name': 'size', 'coefficient' : 0.31, 'z-value': 2.1, 'p-value' : 0.030},
		{ 'name': 'assets', 'coefficient' : 0.23, 'z-value': 0.8, 'p-value' : 0.421}
	],
	'numObs' : 100,
	'RSquared' : 0.31
}

column2 = {
	'counter' : 2,	
	'estimates' : [
		{ 'name': 'size', 'coefficient' : 0.34, 'z-value': 2.0, 'p-value' : 0.040},
		{ 'name': 'assets', 'coefficient' : 0.22, 'z-value': 1.1, 'p-value' : 0.340},
		{ 'name': 'mtb', 'coefficient' : 0.40, 'z-value': 1.4, 'p-value' : 0.121}
	],
	'numObs' : 100,
	'RSquared' : 0.34
}

table = [ column1, column2 ]
```